
* installation
** pesudo
   - if hostname is 127.0.1.1, remember to set it as 127.0.0.1, let it
     same as localhost.
     #+BEGIN_EXAMPLE
     <configuration>
       <property>
         <name>hbase.rootdir</name>
         <value>hdfs://localhost:8020/hbase</value>
       </property>
       <property>
         <name>dfs.replication</name>
         <value>1</value>
       </property>
     </configuration>
     #+END_EXAMPLE

* configuration
** hbase-env.sh
   - pid
     default is /tmp/xxx, this will be cleaned automatically, so
     change it.
* NTP
  if time diff between master and regionsever is to large,
  regionserver won't start!
  use NTP to synchronize time
  : sudo apt-get install ntp
  : sudo ntpdate cn.pool.ntp.org

* Error
** can't drop a table
   : hadoop fs -ls /hbase/
   : hadoop fs -mv /hbase/<table_name> /tmp
   : ./bin/hbase hbck -fixMeta -fixAssignments
* shell
** truncate
   disable, drop, recreate
** deleteall
    only delete a row's column
** compress
   - to set up support for compression, /must/ install the *native binary*
     library on all region servers, and hbase will call them by JNI; if not, you
     will see "Got brand-new compressor" in logfiles, this indicates a failure
     to load the native version while falling back to the Java code
     implementation instead.
   - available compression algrotihms: "GZ", "LZO", "SNAPPY"
   - after issue the compress operation, all the stored files won't change
     unless you issue =major_compact=

   #+BEGIN_SRC shell
   disable table
   alter table, {NAME => "cf", COMPRESSION => "snappy"}
   enable table
   # or compress it when create the table
   create table, {NAME => "CF", COMPRESSION => "snappy"}
   #+END_SRC
* submit job
  To submit job that connect to hbase, beside add hbase jars to
  CLASSPATH, need to add $HABSE_HOME/conf/
* sth
  - hbase won't insert data if the column value if not provided (only
    rowkey is provided)
