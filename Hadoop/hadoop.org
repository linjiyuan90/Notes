  | url             | descritption      |
  |-----------------+-------------------|
  | localhost:50070 |                   |
  | localhost:50075 | hdfs file browser |
  
* distributed cache  
  two ways
  - add -files
  - by programing
** symbol link
   : xx.txt#xx
** recommendation
   put the read only file to hdfs, then call 
   : -files hdfs:///path/cache
** archives
   using =-archives xx.zip=, hadoop will *unarchive* the file, and create a file
   name "xx.zip". To access files in the archive, just 
   : xx.zip/a
   You can create a symbol link by
   : -archives xx.zip#xx
   : # To access the file
   : xx/a
   

* security
** SLA
  *Service Level Authorization* is the initial authorization mechanism
  to ensure clients connecting to a particular Hadoop service have the
  necessary, pre-configured, permissions and are authorized to access
  the given service. For example, a MapReduce cluster can use this
  mechanism to allow a configured list of users/groups to submit jobs.

  The ${HADOOP_CONF_DIR}/hadoop-policy.xml configuration file is used
  to define the access control lists for various Hadoop services.

  Service Level Authorization is performed much before to other access
  control checks such as file-permission checks, access control on job
  queues etc.
** ACL
   access control list

* api
** org.apache.hadoop.conf.Configuration
   : conf.set(key, value);
   : conf.setStrings(key, values);
* jar
  To use generic options like =libjars=, need to make sure the code
  is using =GenericOptionsParser=
** note
   =libjars= are only for mapper and reducer, not for =Job=. To add jars for
   =Job=, set =HADOOP_CLASSPATH=.
* access from edge node
  just change conf to cluster conf, same as hbase. Alternatives is to change
  HADOOP_CONF_DIR, HBASE_CONF_DIR. (Also, note the default conf is in
  $HADOOP_HOME/conf, $HBASE_HOME/conf)
** code
   just change the addResource to configuration
   : val hconf = new Configuraiton()
   : hconf.addResource("conf/hbase-site.xml")
   : hconf.addResource("conf/core-site.xml")
   : hconf.addResource("conf/mapred-site.xml")
   : val pool = new HTablePool(hconf, 10)
   Also, note that since the config files are located by resouce, need to add
   path to CLASSPATH (such as current working path '.', by default it exists)
* other
** tmp file
  It's better not put tmp(like pid) file in /tmp, since this may be removed by
  system.
  : export HADOOP_PID_DIR=
** not running, retry
   if you find log telling you that master(hadoop or hbase) is not running, or
   can't connect to port, first make sure you can telnet it, then check =hosts=
   at every node!  Hadoop register host rather than ip, so you need all the
   corresponding host in your hosts file. Or reverse dns should be setup, so
   that a node's hostname can be looked up through the IP address.
** out-of disk
   When out-of disk, the namenode may crash and no able to start again with log:
   : NameNode: java.lang.NumberFormatException: For input string:â€œ
   One method is to copy files in secondarynamenode to =dfs.name.dir=, then start.
** specify number of mappers
   Hadoop only considers the value of =mapred.map.tasks= as a hint, while
   =mapred.min.split.size= and =mapred.max.split.size= plays a role. However,
   if =mapred.map.tasks= is larger than the number of splits, hadoop will use =mapred.map.tasks=.
** skip bad records
   =mapred.skip.attempts.to.start.skipping=, The number of Task attempts AFTER
   which skip mode will be kicked off;
   =mapred.skip.map.max.skip.records=, The number of acceptable skip records
   surrounding the bad record PER bad record in mapper. If Long.MAX_VALUE,
   means all the remaining records will be droped. If 1, only this bad record
   will be droped.

