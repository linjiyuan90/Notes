# kmeans
* Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value
* iteration stops when centroids move less than the tolerance
* Given enough time, K-means will always converge, however this may be to a local minimum. As a result, the computation is often done several times, with different initializations of the centroids.
* kmeans++:  This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization.
* parallel
* pair of points between clusters may be closer than pair of points in one cluster
* with **density points** induces a partition of the observations corresponding to a Voronoi tessellation generated by the K centroids
[kmeans](http://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/)

* determine the K

## Lloyd' alogrithm
* converges to local minimum (guarantee convergence)
* in practice, run multiple times and averaged

## mini-batch kmeans
## kmeans with SGD
## A Bayesian view (Expectation Maximaztion)


# Spectral Clustering
* close points will aways connect together, lead to a long chain

# Affinity propagation

# Dirichlet Process Mixture Model

# DBSCAN


# Measurement
* Purity
  The issue is that high purity is easy to achieve when the number of cluster is
  large - in particular, purity is 1 if each document gets its own
  cluster. **Not normalized**
* Normalized mutal information

* Rand Index


# Stream Clustering
* single-pass
